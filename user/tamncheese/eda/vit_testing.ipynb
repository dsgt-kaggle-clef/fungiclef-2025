{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af4c364ffebcfe5b",
   "metadata": {},
   "source": [
    "<h1>VIT Testing - tamncheese Jason Kahei Tam<h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca7defede071a58",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "febbe56870545ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:57:57.149338Z",
     "start_time": "2025-02-17T20:57:56.933953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2883270542-366099.jpg', '2237857759-75860.jpg']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import io\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForImageClassification, AutoImageProcessor\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "path_to_images = \"/storage/scratch1/5/jtam30/fungi2024\"\n",
    "files = os.listdir(\"/storage/scratch1/5/jtam30/fungi2024\")\n",
    "print(files[:2])\n",
    "path_to_images_val = \"/storage/scratch1/5/jtam30/fungi2024\"\n",
    "# path_to_images_val = \"/storage/scratch1/5/jtam30/valfungi2024\"\n",
    "# files_val = os.listdir(\"/storage/scratch1/5/jtam30/valfungi2024\")\n",
    "# print(files_val[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bd5e1636bcf9ce",
   "metadata": {},
   "source": [
    "CPU or CUDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "700d4098397b710d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:58:00.777677Z",
     "start_time": "2025-02-17T20:58:00.769123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f6946ff5c956d9",
   "metadata": {},
   "source": [
    "Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eab09158622b64e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:59:30.010174Z",
     "start_time": "2025-02-17T20:59:29.843414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset has number of labels: 1261\n",
      "Validation Dataset has number of labels: 1261\n"
     ]
    }
   ],
   "source": [
    "file_list = []\n",
    "for (\n",
    "    root,\n",
    "    dirs,\n",
    "    files,\n",
    ") in os.walk(\"testing\"):\n",
    "    for file in files:\n",
    "        file_list.append(file)\n",
    "file_list\n",
    "\n",
    "train_df = pd.read_csv(\n",
    "    \"/storage/home/hcoda1/5/jtam30/clef/fungiclef-2025/user/tamncheese/_dataset_trial/metadata_trial.csv\"\n",
    ")\n",
    "\n",
    "# train_df = pd.read_csv(\n",
    "#     \"/storage/scratch1/5/jtam30/FungiCLEF2023_train_metadata_PRODUCTION.csv\"\n",
    "# )\n",
    "train_df = train_df.dropna(subset=[\"species\"]).reset_index(drop=True)\n",
    "\n",
    "n_labels = train_df[\"species\"].nunique()\n",
    "print(f\"Training Dataset has number of labels: {n_labels}\")\n",
    "\n",
    "val_df = pd.read_csv(\n",
    "    \"/storage/home/hcoda1/5/jtam30/clef/fungiclef-2025/user/tamncheese/_dataset_trial/metadata_trial.csv\"\n",
    ")\n",
    "\n",
    "# val_df = pd.read_csv(\n",
    "#     \"/storage/scratch1/5/jtam30/FungiCLEF2023_val_metadata_PRODUCTION.csv\"\n",
    "# )\n",
    "\n",
    "val_df = val_df.dropna(subset=[\"species\"]).reset_index(drop=True)\n",
    "\n",
    "n_labels_val = val_df[\"species\"].nunique()\n",
    "print(f\"Validation Dataset has number of labels: {n_labels_val}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c16cb13",
   "metadata": {},
   "source": [
    "Define FungiDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c6be050c33587a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:59:33.076463Z",
     "start_time": "2025-02-17T20:59:33.070947Z"
    }
   },
   "outputs": [],
   "source": [
    "class FungiDataset(Dataset):\n",
    "    def __init__(self, df, extractor, transform=None, local_filepath=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.extractor = extractor\n",
    "        self.local_filepath = local_filepath\n",
    "        self.label2id = {\n",
    "            label: idx for idx, label in enumerate(sorted(self.df[\"species\"].unique()))\n",
    "        }  # convert labels to integers\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # label = self.df.iloc[idx][\"species\"]\n",
    "        # print(self.local_filepath)\n",
    "        if self.local_filepath:\n",
    "            # print(\"path to image is provided\")\n",
    "            if \"val\" not in self.local_filepath:\n",
    "                img_path = os.path.join(\n",
    "                    self.local_filepath,\n",
    "                    self.df[\"image_path\"].values[idx].replace(\"JPG\", \"jpg\"),\n",
    "                )\n",
    "            else:\n",
    "                img_path = os.path.join(\n",
    "                    self.local_filepath,\n",
    "                    self.df[\"image_path\"].values[idx],\n",
    "                )\n",
    "\n",
    "            # print(img_path)\n",
    "            species_name = self.df.iloc[idx][\"species\"]\n",
    "            label = self.label2id[species_name]\n",
    "            try:\n",
    "                # Load Images (OpenCV)\n",
    "                image = cv2.imread(img_path)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            except Exception as e:\n",
    "                print(f\"Missing image: {img_path}: {e}\")\n",
    "                image = np.random.uniform(-1, 1, size=(299, 299, 3)).astype(np.float32)\n",
    "        else:\n",
    "            print(\"no path is provided\")\n",
    "            image = Image.open(io.BytesIO(self.df.data.values[idx]))\n",
    "            image = np.array(image)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented[\"image\"]\n",
    "\n",
    "        image = self.extractor(images=image, return_tensors=\"pt\")[\n",
    "            \"pixel_values\"\n",
    "        ].squeeze(0)\n",
    "        return image, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e493712d773ec43",
   "metadata": {},
   "source": [
    "Configure DINOv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae2cfbd626deee0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:59:40.413443Z",
     "start_time": "2025-02-17T20:59:39.939213Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Some weights of Dinov2ForImageClassification were not initialized from the model checkpoint at facebook/dinov2-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "extractor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    \"facebook/dinov2-base\", num_labels=n_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c2f72e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Layers\n",
      "dinov2.embeddings.cls_token is trainable\n",
      "dinov2.embeddings.mask_token is trainable\n",
      "dinov2.embeddings.position_embeddings is trainable\n",
      "dinov2.embeddings.patch_embeddings.projection.weight is trainable\n",
      "dinov2.embeddings.patch_embeddings.projection.bias is trainable\n",
      "dinov2.encoder.layer.0.norm1.weight is trainable\n",
      "dinov2.encoder.layer.0.norm1.bias is trainable\n",
      "dinov2.encoder.layer.0.attention.attention.query.weight is trainable\n",
      "dinov2.encoder.layer.0.attention.attention.query.bias is trainable\n",
      "dinov2.encoder.layer.0.attention.attention.key.weight is trainable\n",
      "dinov2.encoder.layer.0.attention.attention.key.bias is trainable\n",
      "dinov2.encoder.layer.0.attention.attention.value.weight is trainable\n",
      "dinov2.encoder.layer.0.attention.attention.value.bias is trainable\n",
      "dinov2.encoder.layer.0.attention.output.dense.weight is trainable\n",
      "dinov2.encoder.layer.0.attention.output.dense.bias is trainable\n",
      "dinov2.encoder.layer.0.layer_scale1.lambda1 is trainable\n",
      "dinov2.encoder.layer.0.norm2.weight is trainable\n",
      "dinov2.encoder.layer.0.norm2.bias is trainable\n",
      "dinov2.encoder.layer.0.mlp.fc1.weight is trainable\n",
      "dinov2.encoder.layer.0.mlp.fc1.bias is trainable\n",
      "dinov2.encoder.layer.0.mlp.fc2.weight is trainable\n",
      "dinov2.encoder.layer.0.mlp.fc2.bias is trainable\n",
      "dinov2.encoder.layer.0.layer_scale2.lambda1 is trainable\n",
      "dinov2.encoder.layer.1.norm1.weight is trainable\n",
      "dinov2.encoder.layer.1.norm1.bias is trainable\n",
      "dinov2.encoder.layer.1.attention.attention.query.weight is trainable\n",
      "dinov2.encoder.layer.1.attention.attention.query.bias is trainable\n",
      "dinov2.encoder.layer.1.attention.attention.key.weight is trainable\n",
      "dinov2.encoder.layer.1.attention.attention.key.bias is trainable\n",
      "dinov2.encoder.layer.1.attention.attention.value.weight is trainable\n",
      "dinov2.encoder.layer.1.attention.attention.value.bias is trainable\n",
      "dinov2.encoder.layer.1.attention.output.dense.weight is trainable\n",
      "dinov2.encoder.layer.1.attention.output.dense.bias is trainable\n",
      "dinov2.encoder.layer.1.layer_scale1.lambda1 is trainable\n",
      "dinov2.encoder.layer.1.norm2.weight is trainable\n",
      "dinov2.encoder.layer.1.norm2.bias is trainable\n",
      "dinov2.encoder.layer.1.mlp.fc1.weight is trainable\n",
      "dinov2.encoder.layer.1.mlp.fc1.bias is trainable\n",
      "dinov2.encoder.layer.1.mlp.fc2.weight is trainable\n",
      "dinov2.encoder.layer.1.mlp.fc2.bias is trainable\n",
      "dinov2.encoder.layer.1.layer_scale2.lambda1 is trainable\n",
      "dinov2.encoder.layer.2.norm1.weight is trainable\n",
      "dinov2.encoder.layer.2.norm1.bias is trainable\n",
      "dinov2.encoder.layer.2.attention.attention.query.weight is trainable\n",
      "dinov2.encoder.layer.2.attention.attention.query.bias is trainable\n",
      "dinov2.encoder.layer.2.attention.attention.key.weight is trainable\n",
      "dinov2.encoder.layer.2.attention.attention.key.bias is trainable\n",
      "dinov2.encoder.layer.2.attention.attention.value.weight is trainable\n",
      "dinov2.encoder.layer.2.attention.attention.value.bias is trainable\n",
      "dinov2.encoder.layer.2.attention.output.dense.weight is trainable\n",
      "dinov2.encoder.layer.2.attention.output.dense.bias is trainable\n",
      "dinov2.encoder.layer.2.layer_scale1.lambda1 is trainable\n",
      "dinov2.encoder.layer.2.norm2.weight is trainable\n",
      "dinov2.encoder.layer.2.norm2.bias is trainable\n",
      "dinov2.encoder.layer.2.mlp.fc1.weight is trainable\n",
      "dinov2.encoder.layer.2.mlp.fc1.bias is trainable\n",
      "dinov2.encoder.layer.2.mlp.fc2.weight is trainable\n",
      "dinov2.encoder.layer.2.mlp.fc2.bias is trainable\n",
      "dinov2.encoder.layer.2.layer_scale2.lambda1 is trainable\n",
      "dinov2.encoder.layer.3.norm1.weight is trainable\n",
      "dinov2.encoder.layer.3.norm1.bias is trainable\n",
      "dinov2.encoder.layer.3.attention.attention.query.weight is trainable\n",
      "dinov2.encoder.layer.3.attention.attention.query.bias is trainable\n",
      "dinov2.encoder.layer.3.attention.attention.key.weight is trainable\n",
      "dinov2.encoder.layer.3.attention.attention.key.bias is trainable\n",
      "dinov2.encoder.layer.3.attention.attention.value.weight is trainable\n",
      "dinov2.encoder.layer.3.attention.attention.value.bias is trainable\n",
      "dinov2.encoder.layer.3.attention.output.dense.weight is trainable\n",
      "dinov2.encoder.layer.3.attention.output.dense.bias is trainable\n",
      "dinov2.encoder.layer.3.layer_scale1.lambda1 is trainable\n",
      "dinov2.encoder.layer.3.norm2.weight is trainable\n",
      "dinov2.encoder.layer.3.norm2.bias is trainable\n",
      "dinov2.encoder.layer.3.mlp.fc1.weight is trainable\n",
      "dinov2.encoder.layer.3.mlp.fc1.bias is trainable\n",
      "dinov2.encoder.layer.3.mlp.fc2.weight is trainable\n",
      "dinov2.encoder.layer.3.mlp.fc2.bias is trainable\n",
      "dinov2.encoder.layer.3.layer_scale2.lambda1 is trainable\n",
      "dinov2.encoder.layer.4.norm1.weight is trainable\n",
      "dinov2.encoder.layer.4.norm1.bias is trainable\n",
      "dinov2.encoder.layer.4.attention.attention.query.weight is trainable\n",
      "dinov2.encoder.layer.4.attention.attention.query.bias is trainable\n",
      "dinov2.encoder.layer.4.attention.attention.key.weight is trainable\n",
      "dinov2.encoder.layer.4.attention.attention.key.bias is trainable\n",
      "dinov2.encoder.layer.4.attention.attention.value.weight is trainable\n",
      "dinov2.encoder.layer.4.attention.attention.value.bias is trainable\n",
      "dinov2.encoder.layer.4.attention.output.dense.weight is trainable\n",
      "dinov2.encoder.layer.4.attention.output.dense.bias is trainable\n",
      "dinov2.encoder.layer.4.layer_scale1.lambda1 is trainable\n",
      "dinov2.encoder.layer.4.norm2.weight is trainable\n",
      "dinov2.encoder.layer.4.norm2.bias is trainable\n",
      "dinov2.encoder.layer.4.mlp.fc1.weight is trainable\n",
      "dinov2.encoder.layer.4.mlp.fc1.bias is trainable\n",
      "dinov2.encoder.layer.4.mlp.fc2.weight is trainable\n",
      "dinov2.encoder.layer.4.mlp.fc2.bias is trainable\n",
      "dinov2.encoder.layer.4.layer_scale2.lambda1 is trainable\n",
      "dinov2.encoder.layer.5.norm1.weight is trainable\n",
      "dinov2.encoder.layer.5.norm1.bias is trainable\n",
      "dinov2.encoder.layer.5.attention.attention.query.weight is trainable\n",
      "dinov2.encoder.layer.5.attention.attention.query.bias is trainable\n",
      "dinov2.encoder.layer.5.attention.attention.key.weight is trainable\n",
      "dinov2.encoder.layer.5.attention.attention.key.bias is trainable\n",
      "dinov2.encoder.layer.5.attention.attention.value.weight is trainable\n",
      "dinov2.encoder.layer.5.attention.attention.value.bias is trainable\n",
      "dinov2.encoder.layer.5.attention.output.dense.weight is trainable\n",
      "dinov2.encoder.layer.5.attention.output.dense.bias is trainable\n",
      "dinov2.encoder.layer.5.layer_scale1.lambda1 is trainable\n",
      "dinov2.encoder.layer.5.norm2.weight is trainable\n",
      "dinov2.encoder.layer.5.norm2.bias is trainable\n",
      "dinov2.encoder.layer.5.mlp.fc1.weight is trainable\n",
      "dinov2.encoder.layer.5.mlp.fc1.bias is trainable\n",
      "dinov2.encoder.layer.5.mlp.fc2.weight is trainable\n",
      "dinov2.encoder.layer.5.mlp.fc2.bias is trainable\n",
      "dinov2.encoder.layer.5.layer_scale2.lambda1 is trainable\n",
      "dinov2.encoder.layer.6.norm1.weight is trainable\n",
      "dinov2.encoder.layer.6.norm1.bias is trainable\n",
      "dinov2.encoder.layer.6.attention.attention.query.weight is trainable\n",
      "dinov2.encoder.layer.6.attention.attention.query.bias is trainable\n",
      "dinov2.encoder.layer.6.attention.attention.key.weight is trainable\n",
      "dinov2.encoder.layer.6.attention.attention.key.bias is trainable\n",
      "dinov2.encoder.layer.6.attention.attention.value.weight is trainable\n",
      "dinov2.encoder.layer.6.attention.attention.value.bias is trainable\n",
      "dinov2.encoder.layer.6.attention.output.dense.weight is trainable\n",
      "dinov2.encoder.layer.6.attention.output.dense.bias is trainable\n",
      "dinov2.encoder.layer.6.layer_scale1.lambda1 is trainable\n",
      "dinov2.encoder.layer.6.norm2.weight is trainable\n",
      "dinov2.encoder.layer.6.norm2.bias is trainable\n",
      "dinov2.encoder.layer.6.mlp.fc1.weight is trainable\n",
      "dinov2.encoder.layer.6.mlp.fc1.bias is trainable\n",
      "dinov2.encoder.layer.6.mlp.fc2.weight is trainable\n",
      "dinov2.encoder.layer.6.mlp.fc2.bias is trainable\n",
      "dinov2.encoder.layer.6.layer_scale2.lambda1 is trainable\n",
      "dinov2.encoder.layer.7.norm1.weight is trainable\n",
      "dinov2.encoder.layer.7.norm1.bias is trainable\n",
      "dinov2.encoder.layer.7.attention.attention.query.weight is trainable\n",
      "dinov2.encoder.layer.7.attention.attention.query.bias is trainable\n",
      "dinov2.encoder.layer.7.attention.attention.key.weight is trainable\n",
      "dinov2.encoder.layer.7.attention.attention.key.bias is trainable\n",
      "dinov2.encoder.layer.7.attention.attention.value.weight is trainable\n",
      "dinov2.encoder.layer.7.attention.attention.value.bias is trainable\n",
      "dinov2.encoder.layer.7.attention.output.dense.weight is trainable\n",
      "dinov2.encoder.layer.7.attention.output.dense.bias is trainable\n",
      "dinov2.encoder.layer.7.layer_scale1.lambda1 is trainable\n",
      "dinov2.encoder.layer.7.norm2.weight is trainable\n",
      "dinov2.encoder.layer.7.norm2.bias is trainable\n",
      "dinov2.encoder.layer.7.mlp.fc1.weight is trainable\n",
      "dinov2.encoder.layer.7.mlp.fc1.bias is trainable\n",
      "dinov2.encoder.layer.7.mlp.fc2.weight is trainable\n",
      "dinov2.encoder.layer.7.mlp.fc2.bias is trainable\n",
      "dinov2.encoder.layer.7.layer_scale2.lambda1 is trainable\n",
      "dinov2.encoder.layer.8.norm1.weight is trainable\n",
      "dinov2.encoder.layer.8.norm1.bias is trainable\n",
      "dinov2.encoder.layer.8.attention.attention.query.weight is trainable\n",
      "dinov2.encoder.layer.8.attention.attention.query.bias is trainable\n",
      "dinov2.encoder.layer.8.attention.attention.key.weight is trainable\n",
      "dinov2.encoder.layer.8.attention.attention.key.bias is trainable\n",
      "dinov2.encoder.layer.8.attention.attention.value.weight is trainable\n",
      "dinov2.encoder.layer.8.attention.attention.value.bias is trainable\n",
      "dinov2.encoder.layer.8.attention.output.dense.weight is trainable\n",
      "dinov2.encoder.layer.8.attention.output.dense.bias is trainable\n",
      "dinov2.encoder.layer.8.layer_scale1.lambda1 is trainable\n",
      "dinov2.encoder.layer.8.norm2.weight is trainable\n",
      "dinov2.encoder.layer.8.norm2.bias is trainable\n",
      "dinov2.encoder.layer.8.mlp.fc1.weight is trainable\n",
      "dinov2.encoder.layer.8.mlp.fc1.bias is trainable\n",
      "dinov2.encoder.layer.8.mlp.fc2.weight is trainable\n",
      "dinov2.encoder.layer.8.mlp.fc2.bias is trainable\n",
      "dinov2.encoder.layer.8.layer_scale2.lambda1 is trainable\n",
      "dinov2.encoder.layer.9.norm1.weight is trainable\n",
      "dinov2.encoder.layer.9.norm1.bias is trainable\n",
      "dinov2.encoder.layer.9.attention.attention.query.weight is trainable\n",
      "dinov2.encoder.layer.9.attention.attention.query.bias is trainable\n",
      "dinov2.encoder.layer.9.attention.attention.key.weight is trainable\n",
      "dinov2.encoder.layer.9.attention.attention.key.bias is trainable\n",
      "dinov2.encoder.layer.9.attention.attention.value.weight is trainable\n",
      "dinov2.encoder.layer.9.attention.attention.value.bias is trainable\n",
      "dinov2.encoder.layer.9.attention.output.dense.weight is trainable\n",
      "dinov2.encoder.layer.9.attention.output.dense.bias is trainable\n",
      "dinov2.encoder.layer.9.layer_scale1.lambda1 is trainable\n",
      "dinov2.encoder.layer.9.norm2.weight is trainable\n",
      "dinov2.encoder.layer.9.norm2.bias is trainable\n",
      "dinov2.encoder.layer.9.mlp.fc1.weight is trainable\n",
      "dinov2.encoder.layer.9.mlp.fc1.bias is trainable\n",
      "dinov2.encoder.layer.9.mlp.fc2.weight is trainable\n",
      "dinov2.encoder.layer.9.mlp.fc2.bias is trainable\n",
      "dinov2.encoder.layer.9.layer_scale2.lambda1 is trainable\n",
      "dinov2.encoder.layer.10.norm1.weight is trainable\n",
      "dinov2.encoder.layer.10.norm1.bias is trainable\n",
      "dinov2.encoder.layer.10.attention.attention.query.weight is trainable\n",
      "dinov2.encoder.layer.10.attention.attention.query.bias is trainable\n",
      "dinov2.encoder.layer.10.attention.attention.key.weight is trainable\n",
      "dinov2.encoder.layer.10.attention.attention.key.bias is trainable\n",
      "dinov2.encoder.layer.10.attention.attention.value.weight is trainable\n",
      "dinov2.encoder.layer.10.attention.attention.value.bias is trainable\n",
      "dinov2.encoder.layer.10.attention.output.dense.weight is trainable\n",
      "dinov2.encoder.layer.10.attention.output.dense.bias is trainable\n",
      "dinov2.encoder.layer.10.layer_scale1.lambda1 is trainable\n",
      "dinov2.encoder.layer.10.norm2.weight is trainable\n",
      "dinov2.encoder.layer.10.norm2.bias is trainable\n",
      "dinov2.encoder.layer.10.mlp.fc1.weight is trainable\n",
      "dinov2.encoder.layer.10.mlp.fc1.bias is trainable\n",
      "dinov2.encoder.layer.10.mlp.fc2.weight is trainable\n",
      "dinov2.encoder.layer.10.mlp.fc2.bias is trainable\n",
      "dinov2.encoder.layer.10.layer_scale2.lambda1 is trainable\n",
      "dinov2.encoder.layer.11.norm1.weight is trainable\n",
      "dinov2.encoder.layer.11.norm1.bias is trainable\n",
      "dinov2.encoder.layer.11.attention.attention.query.weight is trainable\n",
      "dinov2.encoder.layer.11.attention.attention.query.bias is trainable\n",
      "dinov2.encoder.layer.11.attention.attention.key.weight is trainable\n",
      "dinov2.encoder.layer.11.attention.attention.key.bias is trainable\n",
      "dinov2.encoder.layer.11.attention.attention.value.weight is trainable\n",
      "dinov2.encoder.layer.11.attention.attention.value.bias is trainable\n",
      "dinov2.encoder.layer.11.attention.output.dense.weight is trainable\n",
      "dinov2.encoder.layer.11.attention.output.dense.bias is trainable\n",
      "dinov2.encoder.layer.11.layer_scale1.lambda1 is trainable\n",
      "dinov2.encoder.layer.11.norm2.weight is trainable\n",
      "dinov2.encoder.layer.11.norm2.bias is trainable\n",
      "dinov2.encoder.layer.11.mlp.fc1.weight is trainable\n",
      "dinov2.encoder.layer.11.mlp.fc1.bias is trainable\n",
      "dinov2.encoder.layer.11.mlp.fc2.weight is trainable\n",
      "dinov2.encoder.layer.11.mlp.fc2.bias is trainable\n",
      "dinov2.encoder.layer.11.layer_scale2.lambda1 is trainable\n",
      "dinov2.layernorm.weight is trainable\n",
      "dinov2.layernorm.bias is trainable\n",
      "classifier.weight is trainable\n",
      "classifier.bias is trainable\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "print(\"Trainable Layers\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name} is trainable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87656311835cf4ce",
   "metadata": {},
   "source": [
    "Frreze Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2887ca368a017dc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:59:44.150519Z",
     "start_time": "2025-02-17T20:59:44.144599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Layers\n",
      "classifier.weight is trainable\n",
      "classifier.bias is trainable\n"
     ]
    }
   ],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"Trainable Layers\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name} is trainable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a4e270a32b844d",
   "metadata": {},
   "source": [
    "Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "683f1a79e46a1816",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T21:02:01.712934Z",
     "start_time": "2025-02-17T21:02:01.708490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_filepath is /storage/scratch1/5/jtam30/fungi2024/\n"
     ]
    }
   ],
   "source": [
    "img_dir = path_to_images + \"/\"\n",
    "print(f\"local_filepath is {img_dir}\")\n",
    "\n",
    "train_dataset = FungiDataset(train_df, extractor, local_filepath=img_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "img_val_dir = path_to_images_val + \"/\"\n",
    "val_dataset = FungiDataset(val_df, extractor, local_filepath=img_val_dir)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26be9e39",
   "metadata": {},
   "source": [
    "Ranked List Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "623d4aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function in progress\n",
    "# softmax the logits and extract the top-k predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eccebcdd27a55d9",
   "metadata": {},
   "source": [
    "Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e2e139d7b1e3cff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:56:43.321075100Z",
     "start_time": "2025-02-17T06:09:09.911896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Training Loss: 7.695611953735352 for epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e84031263043a589a9e807c6b55ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.5608, Accuracy: 0.10%\n",
      "\n",
      "Epoch 2/10\n",
      "Training Loss: 7.501148568822982 for epoch 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb259c4295aa4fb08a618f4c8a9c5f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.3823, Accuracy: 0.30%\n",
      "\n",
      "Epoch 3/10\n",
      "Training Loss: 7.33756755260711 for epoch 3/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2e6e9b403749adad2ca3fe0197cc36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.2318, Accuracy: 0.44%\n",
      "\n",
      "Epoch 4/10\n",
      "Training Loss: 7.184212065757589 for epoch 4/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718f4b75316041a5a391d3350661e0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.0865, Accuracy: 0.61%\n",
      "\n",
      "Epoch 5/10\n",
      "Training Loss: 7.043139508429994 for epoch 5/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427ebca49fdb4b8c830e7d17dbc04929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.9404, Accuracy: 1.05%\n",
      "\n",
      "Epoch 6/10\n",
      "Training Loss: 6.894737953835345 for epoch 6/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9318067253a64245a87fd0ff32d01c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.8071, Accuracy: 1.28%\n",
      "\n",
      "Epoch 7/10\n",
      "Training Loss: 6.775538890919787 for epoch 7/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9312444bec2946979dd5d38faa9a115d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.6754, Accuracy: 1.35%\n",
      "\n",
      "Epoch 8/10\n",
      "Training Loss: 6.638925156694778 for epoch 8/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ba1702dbe848d799677dc696fe5628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.5579, Accuracy: 1.99%\n",
      "\n",
      "Epoch 9/10\n",
      "Training Loss: 6.519043790533187 for epoch 9/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9883e12b0dcc4132af0942674e638c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.4392, Accuracy: 2.20%\n",
      "\n",
      "Epoch 10/10\n",
      "Training Loss: 6.407730975049607 for epoch 10/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2d4f8de8664b27b601f1e8741589ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.3244, Accuracy: 2.50%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), lr=0.00001\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        # print(images)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Training Loss: {avg_train_loss} for epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbd4aa22",
   "metadata": {},
   "source": [
    "Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3f69bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    \"/storage/home/hcoda1/5/jtam30/clef/fungiclef-2025/user/tamncheese/_dataset_trial/model_trial.pth\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3ac4d54",
   "metadata": {},
   "source": [
    "Extract Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc6bd294",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mfacebook/dinov2-base\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m model\u001b[39m.\u001b[39meval()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m features \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoModel' is not defined"
     ]
    }
   ],
   "source": [
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    \"facebook/dinov2-base\", num_labels=n_labels\n",
    ")\n",
    "model.eval().to(device)\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, label in train_loader:\n",
    "        image_list = [image.permute(1, 2, 0).cpu().numpy() for image in images]\n",
    "\n",
    "        # Extract features for batch\n",
    "        # batch_features = np.vstack([extract_features(model, img) for img in image_list])\n",
    "        # features.append(batch_features)\n",
    "        labels.append(label.numpy())\n",
    "\n",
    "features = np.vstack(features)\n",
    "labels = np.hstack(labels)\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(features, labels)\n",
    "\n",
    "test_img_path = \"/storage/scratch1/5/jtam30/fungi2024/2883270542-366099.jpg\"\n",
    "image = cv2.imread(test_img_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "inputs = extractor(images=image, return_tensors=\"pt\")\n",
    "pixel_values = inputs[\"pixel_values\"].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract the CLS token feature\n",
    "features = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "class_names = {v: k for k, v in train_dataset.label2id.items()}\n",
    "class_names = [class_names[i] for i in range(len(class_names))]\n",
    "\n",
    "predicted_label = log_reg.predict(features)[0]\n",
    "predicted_species = class_names[predicted_label]\n",
    "\n",
    "print(predicted_species)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
