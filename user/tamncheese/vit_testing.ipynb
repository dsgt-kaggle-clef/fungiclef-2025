{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af4c364ffebcfe5b",
   "metadata": {},
   "source": [
    "<h1>VIT Testing - tamncheese Jason Kahei Tam<h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca7defede071a58",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "febbe56870545ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:57:57.149338Z",
     "start_time": "2025-02-17T20:57:56.933953Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForImageClassification, AutoImageProcessor\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import io\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForImageClassification, AutoImageProcessor\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path_to_images = \"../../../../scratch/fungi2024\"\n",
    "files = os.listdir(\"../../../../scratch/fungi2024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bd5e1636bcf9ce",
   "metadata": {},
   "source": [
    "CPU or CUDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "700d4098397b710d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:58:00.777677Z",
     "start_time": "2025-02-17T20:58:00.769123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f6946ff5c956d9",
   "metadata": {},
   "source": [
    "Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eab09158622b64e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:59:30.010174Z",
     "start_time": "2025-02-17T20:59:29.843414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 1577\n"
     ]
    }
   ],
   "source": [
    "file_list = []\n",
    "for (\n",
    "    root,\n",
    "    dirs,\n",
    "    files,\n",
    ") in os.walk(\"testing\"):\n",
    "    for file in files:\n",
    "        file_list.append(file)\n",
    "file_list\n",
    "\n",
    "train_df = pd.read_csv(\"_dataset_trial/metadata_trial.csv\")\n",
    "n_labels = train_df[\"species\"].nunique()\n",
    "print(f\"Number of labels: {n_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c6be050c33587a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:59:33.076463Z",
     "start_time": "2025-02-17T20:59:33.070947Z"
    }
   },
   "outputs": [],
   "source": [
    "class FungiDataset(Dataset):\n",
    "    def __init__(self, df, extractor, transform=None, local_filepath=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.extractor = extractor\n",
    "        self.local_filepath = local_filepath\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.df.iloc[idx][\"species\"]\n",
    "        if self.local_filepath:\n",
    "            img_path = self.local_filepath + self.df[\"image_path\"].values[idx].replace(\n",
    "                \"jpg\", \"JPG\"\n",
    "            )\n",
    "            try:\n",
    "                # Load Images (OpenCV)\n",
    "                image = cv2.imread(img_path)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            except Exception as e:\n",
    "                print(f\"Missing image: {img_path}: {e}\")\n",
    "                image = np.random.uniform(-1, 1, size=(299, 299, 3)).astype(np.float32)\n",
    "        else:\n",
    "            image = Image.open(io.BytesIO(self.df.data.values[idx]))\n",
    "            image = np.array(image)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented[\"image\"]\n",
    "\n",
    "        image = self.extractor(images=image, return_tensors=\"pt\")[\n",
    "            \"pixel_values\"\n",
    "        ].squeeze(0)\n",
    "        return image, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e493712d773ec43",
   "metadata": {},
   "source": [
    "Configure DINOv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae2cfbd626deee0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:59:40.413443Z",
     "start_time": "2025-02-17T20:59:39.939213Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Some weights of Dinov2ForImageClassification were not initialized from the model checkpoint at facebook/dinov2-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "extractor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    \"facebook/dinov2-base\", num_labels=n_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87656311835cf4ce",
   "metadata": {},
   "source": [
    "Frreze Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2887ca368a017dc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:59:44.150519Z",
     "start_time": "2025-02-17T20:59:44.144599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Layers\n",
      "classifier.weight is trainable\n",
      "classifier.bias is trainable\n"
     ]
    }
   ],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"Trainable Layers\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name} is trainable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a4e270a32b844d",
   "metadata": {},
   "source": [
    "Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1749df078abd832b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "683f1a79e46a1816",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T21:02:01.712934Z",
     "start_time": "2025-02-17T21:02:01.708490Z"
    }
   },
   "outputs": [],
   "source": [
    "img_dir = \"training_laptop\"\n",
    "train_dataset = FungiDataset(train_df, extractor, img_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eccebcdd27a55d9",
   "metadata": {},
   "source": [
    "Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e2e139d7b1e3cff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:56:43.321075100Z",
     "start_time": "2025-02-17T06:09:09.911896Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m p: p\u001b[38;5;241m.\u001b[39mrequires_grad, model\u001b[38;5;241m.\u001b[39mparameters()), lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.00001\u001b[39m)\n\u001b[0;32m      2\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m      4\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), lr=0.00001\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Training Loss: {avg_train_loss} for epoch {epoch + 1}/{epochs}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
