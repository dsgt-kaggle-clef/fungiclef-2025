{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af4c364ffebcfe5b",
   "metadata": {},
   "source": [
    "<h1>VIT Testing - tamncheese Jason Kahei Tam<h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca7defede071a58",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "febbe56870545ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:57:57.149338Z",
     "start_time": "2025-02-17T20:57:56.933953Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/hcoda1/5/jtam30/clef/fungiclef-2025/.venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2883270542-366099.jpg', '2237857759-75860.jpg']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import io\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForImageClassification, AutoImageProcessor\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path_to_images = \"/storage/scratch1/5/jtam30/fungi2024\"\n",
    "files = os.listdir(\"/storage/scratch1/5/jtam30/fungi2024\")\n",
    "print(files[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bd5e1636bcf9ce",
   "metadata": {},
   "source": [
    "CPU or CUDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "700d4098397b710d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:58:00.777677Z",
     "start_time": "2025-02-17T20:58:00.769123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f6946ff5c956d9",
   "metadata": {},
   "source": [
    "Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eab09158622b64e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:59:30.010174Z",
     "start_time": "2025-02-17T20:59:29.843414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 1261\n"
     ]
    }
   ],
   "source": [
    "file_list = []\n",
    "for (\n",
    "    root,\n",
    "    dirs,\n",
    "    files,\n",
    ") in os.walk(\"testing\"):\n",
    "    for file in files:\n",
    "        file_list.append(file)\n",
    "file_list\n",
    "\n",
    "train_df = pd.read_csv(\"_dataset_trial/metadata_trial.csv\")\n",
    "n_labels = train_df[\"species\"].nunique()\n",
    "print(f\"Number of labels: {n_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c6be050c33587a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:59:33.076463Z",
     "start_time": "2025-02-17T20:59:33.070947Z"
    }
   },
   "outputs": [],
   "source": [
    "class FungiDataset(Dataset):\n",
    "    def __init__(self, df, extractor, transform=None, local_filepath=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.extractor = extractor\n",
    "        self.local_filepath = local_filepath\n",
    "        self.label2id = {\n",
    "            label: idx for idx, label in enumerate(sorted(self.df[\"species\"].unique()))\n",
    "        }  # convert labels to integers\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # label = self.df.iloc[idx][\"species\"]\n",
    "        # print(self.local_filepath)\n",
    "        if self.local_filepath:\n",
    "            # print(\"path to image is provided\")\n",
    "            img_path = os.path.join(\n",
    "                self.local_filepath,\n",
    "                self.df[\"image_path\"].values[idx].replace(\"JPG\", \"jpg\"),\n",
    "            )\n",
    "            # print(img_path)\n",
    "            species_name = self.df.iloc[idx][\"species\"]\n",
    "            label = self.label2id[species_name]\n",
    "            try:\n",
    "                # Load Images (OpenCV)\n",
    "                image = cv2.imread(img_path)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            except Exception as e:\n",
    "                print(f\"Missing image: {img_path}: {e}\")\n",
    "                image = np.random.uniform(-1, 1, size=(299, 299, 3)).astype(np.float32)\n",
    "        else:\n",
    "            print(\"no path is provided\")\n",
    "            image = Image.open(io.BytesIO(self.df.data.values[idx]))\n",
    "            image = np.array(image)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented[\"image\"]\n",
    "\n",
    "        image = self.extractor(images=image, return_tensors=\"pt\")[\n",
    "            \"pixel_values\"\n",
    "        ].squeeze(0)\n",
    "        return image, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e493712d773ec43",
   "metadata": {},
   "source": [
    "Configure DINOv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae2cfbd626deee0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:59:40.413443Z",
     "start_time": "2025-02-17T20:59:39.939213Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Dinov2ForImageClassification were not initialized from the model checkpoint at facebook/dinov2-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "extractor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    \"facebook/dinov2-base\", num_labels=n_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c2f72e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87656311835cf4ce",
   "metadata": {},
   "source": [
    "Frreze Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2887ca368a017dc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:59:44.150519Z",
     "start_time": "2025-02-17T20:59:44.144599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Layers\n",
      "classifier.weight is trainable\n",
      "classifier.bias is trainable\n"
     ]
    }
   ],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"Trainable Layers\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name} is trainable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a4e270a32b844d",
   "metadata": {},
   "source": [
    "Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "683f1a79e46a1816",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T21:02:01.712934Z",
     "start_time": "2025-02-17T21:02:01.708490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/scratch1/5/jtam30/fungi2024\n",
      "local_filepath is /storage/scratch1/5/jtam30/fungi2024/\n"
     ]
    }
   ],
   "source": [
    "img_dir = path_to_images + \"/\"\n",
    "print(path_to_images)\n",
    "print(f\"local_filepath is {img_dir}\")\n",
    "\n",
    "train_dataset = FungiDataset(train_df, extractor, local_filepath=img_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eccebcdd27a55d9",
   "metadata": {},
   "source": [
    "Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e2e139d7b1e3cff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:56:43.321075100Z",
     "start_time": "2025-02-17T06:09:09.911896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Training Loss: 6.910690825036231 for epoch 1/10\n",
      "Epoch 2/10\n",
      "Training Loss: 6.7553349657261625 for epoch 2/10\n",
      "Epoch 3/10\n",
      "Training Loss: 6.637313010844778 for epoch 3/10\n",
      "Epoch 4/10\n",
      "Training Loss: 6.514225574249917 for epoch 4/10\n",
      "Epoch 5/10\n",
      "Training Loss: 6.398634565637467 for epoch 5/10\n",
      "Epoch 6/10\n",
      "Training Loss: 6.296313671355552 for epoch 6/10\n",
      "Epoch 7/10\n",
      "Training Loss: 6.182277212751672 for epoch 7/10\n",
      "Epoch 8/10\n",
      "Training Loss: 6.083072205807301 for epoch 8/10\n",
      "Epoch 9/10\n",
      "Training Loss: 5.979990857712766 for epoch 9/10\n",
      "Epoch 10/10\n",
      "Training Loss: 5.8737304464299624 for epoch 10/10\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), lr=0.00001\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        # print(images)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Training Loss: {avg_train_loss} for epoch {epoch + 1}/{epochs}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbd4aa22",
   "metadata": {},
   "source": [
    "Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3f69bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    \"/storage/home/hcoda1/5/jtam30/clef/fungiclef-2025/user/tamncheese/_dataset_trial/model_trial.pth\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3ac4d54",
   "metadata": {},
   "source": [
    "Extract Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc6bd294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dinov2ForImageClassification(\n",
       "  (dinov2): Dinov2Model(\n",
       "    (embeddings): Dinov2Embeddings(\n",
       "      (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Dinov2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x Dinov2Layer(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attention): Dinov2SdpaAttention(\n",
       "            (attention): Dinov2SdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): Dinov2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (layer_scale1): Dinov2LayerScale()\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Dinov2MLP(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_scale2): Dinov2LayerScale()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1536, out_features=1261, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
