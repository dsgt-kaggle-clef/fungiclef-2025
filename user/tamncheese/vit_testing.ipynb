{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af4c364ffebcfe5b",
   "metadata": {},
   "source": [
    "<h1>VIT Testing - tamncheese Jason Kahei Tam<h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca7defede071a58",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "febbe56870545ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:57:57.149338Z",
     "start_time": "2025-02-17T20:57:56.933953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2883270542-366099.jpg', '2237857759-75860.jpg']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import io\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForImageClassification, AutoImageProcessor\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "path_to_images = \"/storage/scratch1/5/jtam30/fungi2024\"\n",
    "files = os.listdir(\"/storage/scratch1/5/jtam30/fungi2024\")\n",
    "print(files[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bd5e1636bcf9ce",
   "metadata": {},
   "source": [
    "CPU or CUDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "700d4098397b710d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:58:00.777677Z",
     "start_time": "2025-02-17T20:58:00.769123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f6946ff5c956d9",
   "metadata": {},
   "source": [
    "Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8eab09158622b64e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:59:30.010174Z",
     "start_time": "2025-02-17T20:59:29.843414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset has number of labels: 1577\n",
      "Validation Dataset has number of labels: 2673\n"
     ]
    }
   ],
   "source": [
    "file_list = []\n",
    "for (\n",
    "    root,\n",
    "    dirs,\n",
    "    files,\n",
    ") in os.walk(\"testing\"):\n",
    "    for file in files:\n",
    "        file_list.append(file)\n",
    "file_list\n",
    "\n",
    "train_df = pd.read_csv(\n",
    "    \"/storage/scratch1/5/jtam30/FungiCLEF2023_train_metadata_PRODUCTION.csv\"\n",
    ")\n",
    "train_df = train_df.dropna(subset=[\"species\"]).reset_index(drop=True)\n",
    "\n",
    "n_labels = train_df[\"species\"].nunique()\n",
    "print(f\"Training Dataset has number of labels: {n_labels}\")\n",
    "\n",
    "val_df = pd.read_csv(\n",
    "    \"/storage/scratch1/5/jtam30/FungiCLEF2023_val_metadata_PRODUCTION.csv\"\n",
    ")\n",
    "val_df = val_df.dropna(subset=[\"species\"]).reset_index(drop=True)\n",
    "\n",
    "n_labels_val = val_df[\"species\"].nunique()\n",
    "print(f\"Validation Dataset has number of labels: {n_labels_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c6be050c33587a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:59:33.076463Z",
     "start_time": "2025-02-17T20:59:33.070947Z"
    }
   },
   "outputs": [],
   "source": [
    "class FungiDataset(Dataset):\n",
    "    def __init__(self, df, extractor, transform=None, local_filepath=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.extractor = extractor\n",
    "        self.local_filepath = local_filepath\n",
    "        self.label2id = {\n",
    "            label: idx for idx, label in enumerate(sorted(self.df[\"species\"].unique()))\n",
    "        }  # convert labels to integers\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # label = self.df.iloc[idx][\"species\"]\n",
    "        # print(self.local_filepath)\n",
    "        if self.local_filepath:\n",
    "            # print(\"path to image is provided\")\n",
    "            img_path = os.path.join(\n",
    "                self.local_filepath,\n",
    "                self.df[\"image_path\"].values[idx].replace(\"JPG\", \"jpg\"),\n",
    "            )\n",
    "            # print(img_path)\n",
    "            species_name = self.df.iloc[idx][\"species\"]\n",
    "            label = self.label2id[species_name]\n",
    "            try:\n",
    "                # Load Images (OpenCV)\n",
    "                image = cv2.imread(img_path)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            except Exception as e:\n",
    "                print(f\"Missing image: {img_path}: {e}\")\n",
    "                image = np.random.uniform(-1, 1, size=(299, 299, 3)).astype(np.float32)\n",
    "        else:\n",
    "            print(\"no path is provided\")\n",
    "            image = Image.open(io.BytesIO(self.df.data.values[idx]))\n",
    "            image = np.array(image)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented[\"image\"]\n",
    "\n",
    "        image = self.extractor(images=image, return_tensors=\"pt\")[\n",
    "            \"pixel_values\"\n",
    "        ].squeeze(0)\n",
    "        return image, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e493712d773ec43",
   "metadata": {},
   "source": [
    "Configure DINOv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae2cfbd626deee0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:59:40.413443Z",
     "start_time": "2025-02-17T20:59:39.939213Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Dinov2ForImageClassification were not initialized from the model checkpoint at facebook/dinov2-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "extractor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    \"facebook/dinov2-base\", num_labels=n_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c2f72e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87656311835cf4ce",
   "metadata": {},
   "source": [
    "Frreze Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2887ca368a017dc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:59:44.150519Z",
     "start_time": "2025-02-17T20:59:44.144599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Layers\n",
      "classifier.weight is trainable\n",
      "classifier.bias is trainable\n"
     ]
    }
   ],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"Trainable Layers\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name} is trainable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a4e270a32b844d",
   "metadata": {},
   "source": [
    "Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "683f1a79e46a1816",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T21:02:01.712934Z",
     "start_time": "2025-02-17T21:02:01.708490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_filepath is /storage/scratch1/5/jtam30/fungi2024/\n"
     ]
    }
   ],
   "source": [
    "img_dir = path_to_images + \"/\"\n",
    "print(f\"local_filepath is {img_dir}\")\n",
    "\n",
    "train_dataset = FungiDataset(train_df, extractor, local_filepath=img_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_dataset = FungiDataset(val_df, extractor, local_filepath=img_dir)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26be9e39",
   "metadata": {},
   "source": [
    "Ranked List Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "623d4aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function in progress\n",
    "# softmax the logits and extract the top-k predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eccebcdd27a55d9",
   "metadata": {},
   "source": [
    "Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e2e139d7b1e3cff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:56:43.321075100Z",
     "start_time": "2025-02-17T06:09:09.911896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Training Loss: 6.910690825036231 for epoch 1/10\n",
      "Epoch 2/10\n",
      "Training Loss: 6.7553349657261625 for epoch 2/10\n",
      "Epoch 3/10\n",
      "Training Loss: 6.637313010844778 for epoch 3/10\n",
      "Epoch 4/10\n",
      "Training Loss: 6.514225574249917 for epoch 4/10\n",
      "Epoch 5/10\n",
      "Training Loss: 6.398634565637467 for epoch 5/10\n",
      "Epoch 6/10\n",
      "Training Loss: 6.296313671355552 for epoch 6/10\n",
      "Epoch 7/10\n",
      "Training Loss: 6.182277212751672 for epoch 7/10\n",
      "Epoch 8/10\n",
      "Training Loss: 6.083072205807301 for epoch 8/10\n",
      "Epoch 9/10\n",
      "Training Loss: 5.979990857712766 for epoch 9/10\n",
      "Epoch 10/10\n",
      "Training Loss: 5.8737304464299624 for epoch 10/10\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), lr=0.00001\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        # print(images)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Training Loss: {avg_train_loss} for epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbd4aa22",
   "metadata": {},
   "source": [
    "Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3f69bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    \"/storage/home/hcoda1/5/jtam30/clef/fungiclef-2025/user/tamncheese/_dataset_trial/model_trial.pth\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3ac4d54",
   "metadata": {},
   "source": [
    "Extract Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6bd294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dinov2ForImageClassification(\n",
       "  (dinov2): Dinov2Model(\n",
       "    (embeddings): Dinov2Embeddings(\n",
       "      (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Dinov2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x Dinov2Layer(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attention): Dinov2SdpaAttention(\n",
       "            (attention): Dinov2SdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): Dinov2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (layer_scale1): Dinov2LayerScale()\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Dinov2MLP(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_scale2): Dinov2LayerScale()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1536, out_features=1261, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, label in train_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model.backbone(images).logits\n",
    "        features.append(outputs.cpu().numpy())\n",
    "        labels.append(label.numpy)\n",
    "\n",
    "features = np.vstack(features)\n",
    "labels = np.hstack(labels)\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(features, labels)\n",
    "\n",
    "test_img_path = \"/storage/scratch1/5/jtam30/fungi2024/2883270542-366099.jpg\"\n",
    "image = cv2.imread(test_img_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "inputs = extractor(images=image, return_tensors=\"pt\")\n",
    "pixel_values = inputs[\"pixel_values\"].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = model.backbone(image).logits\n",
    "features = features.cpu().numpy()\n",
    "\n",
    "\n",
    "class_names = {v: k for k, v in train_dataset.label2id.items()}\n",
    "class_names = [class_names[i] for i in range(len(class_names))]\n",
    "\n",
    "predicted_label = log_reg.predict(features)[0]\n",
    "predicted_species = class_names[predicted_label]\n",
    "\n",
    "print(predicted_species)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
